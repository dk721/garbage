## **1. Основные понятия и структурные схемы**  
### **1. Понятие информации. Структурная схема одноканальной системы передачи информации**  
- **Информация** – мера уменьшения неопределенности (по Шеннону).  
- **Структурная схема системы передачи**:  
  ```
  Источник → Кодирующее устройство → Канал связи → Декодирующее устройство → Получатель
  ```  
  (Могут добавляться модуляция, демодуляция, помехи.)  

### **2. Определение дискретных ансамблей и источников сообщений**  
- **Дискретный ансамбль** – множество возможных сообщений с заданными вероятностями.  
- **Источник сообщений** – устройство или процесс, генерирующий сообщения (с памятью/без памяти).  

---

## **2. Меры информации и энтропия**  
### **3. Количество информации в дискретном сообщении**  
- **Формула Шеннона**:  
  \[
  I(a_i) = -\log_2 P(a_i)
  \]  
  **Обоснование логарифма**: аддитивность для независимых событий.  

### **4. Энтропия ансамбля дискретных сообщений**  
\[
H(X) = -\sum_{i=1}^n P(x_i) \log_2 P(x_i)
\]  
**Свойства**:  
- \( H(X) \geq 0 \)  
- Максимальна при равномерном распределении.  

### **5. Условная и совместная энтропия**  
- **Условная**: \( H(X|Y) = -\sum P(x,y) \log P(x|y) \)  
- **Совместная**: \( H(X,Y) = H(X) + H(Y|X) \)  

### **6. Энтропия стационарного источника**  
Для источника с памятью:  
\[
H_\infty = \lim_{n \to \infty} \frac{H(X_1, X_2, ..., X_n)}{n}
\]  

### **7. Избыточность источника**  
\[
R = 1 - \frac{H(X)}{H_{max}}
\]  
**Безызбыточный источник**: \( H(X) = H_{max} \) (все символы равновероятны).  

---

## **3. Кодирование информации**  
### **8. Эффективное кодирование**  
Цель: минимизировать среднюю длину кода при условии однозначности декодирования.  

### **9. Равномерное кодирование**  
- Все кодовые слова одинаковой длины (например, ASCII).  

### **10–11. Теорема о высоковероятных множествах. Типичные и нетипичные последовательности**  
- **Типичные последовательности** – их вероятность близка к \( 2^{-nH(X)} \).  
- **Нетипичные** – все остальные (их вероятность мала).  

### **12. Энтропия как скорость создания информации**  
Для источника без памяти:  
\[
H(X) \text{ [бит/символ]} = \text{скорость генерации информации}
\]  

### **13–15. Неравномерные коды**  
- **Код Шеннона-Фано**: разделение вероятностей на две группы.  
- **Код Хаффмана**: оптимальный префиксный код.  
- **Арифметическое кодирование** (Гилберт-Мур): кодирование всей последовательности в одно число.  

---

## **4. Каналы связи и пропускная способность**  
### **16. Классификация каналов**  
- Дискретные/непрерывные, с памятью/без памяти, с помехами/без помех.  

### **17–18. Количество информации и взаимная информация**  
\[
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
\]  
**Свойства**:  
- \( I(X;Y) \geq 0 \)  
- Симметричность: \( I(X;Y) = I(Y;X) \)  

### **19. Информационные характеристики системы передачи**  
- **Скорость передачи** \( R \)  
- **Пропускная способность** \( C \)  

### **20. Пропускная способность канала**  
Для дискретного канала без памяти:  
\[
C = \max_{P(x)} I(X;Y)
\]  

### **21–22. Симметричные каналы**  
- **Двоичный симметричный канал (ДСК)**:  
  \[
  C = 1 - H(p), \text{ где } p \text{ – вероятность ошибки}
  \]  
- **ДСК со стиранием**:  
  \[
  C = 1 - \alpha, \text{ где } \alpha \text{ – вероятность стирания}
  \]  

### **23–25. Теоремы Шеннона о кодировании**  
- **Прямая теорема**: Если \( R < C \), то существует код с вероятностью ошибки \( \rightarrow 0 \).  
- **Обратная теорема**: Если \( R > C \), то надежная передача невозможна.  

### **26. Помехоустойчивое кодирование**  
- **Коды Хэмминга, циклические коды, свёрточные коды**.  

---

## **5. Непрерывные источники и каналы**  
### **27–28. Непрерывные ансамбли**  
- **Дифференциальная энтропия**:  
  \[
  h(X) = -\int f(x) \log f(x) \, dx
  \]  
- **Взаимная информация для непрерывных сигналов**:  
  \[
  I(X;Y) = h(X) - h(X|Y)
  \]  

### **29. Пропускная способность непрерывного канала**  
**Формула Шеннона-Хартли**:  
\[
C = B \log_2 \left(1 + \frac{S}{N}\right)
\]  
где \( B \) – полоса пропускания, \( S/N \) – отношение сигнал/шум.  

---

## **6. Теоремы кодирования и эффективность систем передачи**  
### **30. Теоремы кодирования для непрерывного канала без памяти с дискретным временем**  
- **Прямая теорема**: Если скорость передачи \( R < C \), то существует код, обеспечивающий сколь угодно малую вероятность ошибки.  
- **Обратная теорема**: Если \( R > C \), то надёжная передача невозможна.  
- **Аналог теорем Шеннона**, но для непрерывных каналов.  

### **31. Эффективность систем передачи и пределы Шеннона**  
- **Эффективность** – отношение фактической скорости передачи к пропускной способности \( \eta = \frac{R}{C} \).  
- **Предел Шеннона** – максимальная скорость безошибочной передачи при заданных ограничениях (мощность, полоса).  

### **32. Критерий качества воспроизведения сообщений**  
- **Искажения**: среднеквадратическая ошибка (MSE), вероятность ошибки.  
- **Критерий Найквиста-Котельникова** – точное восстановление при частоте дискретизации \( f_s \geq 2f_{max} \).  

### **33. Теорема Котельникова (Найквиста-Шеннона)**  
- **Формулировка**:  
  > Аналоговый сигнал с ограниченным спектром \( f_{max} \) может быть точно восстановлен по дискретным отсчётам, взятым с частотой \( f_s \geq 2f_{max} \).  

- **Значение**: основа всех цифровых систем связи.  

---

## **7. Коды и неравенства**  
### **34. Неравенство Крафта**  
- **Формулировка**:  
  Для любого префиксного кода с длинами кодовых слов \( l_1, l_2, ..., l_n \) выполняется:  
  \[
  \sum_{i=1}^n 2^{-l_i} \leq 1
  \]  
- **Применение**: проверка существования префиксного кода.  

### **35–36. Цепи Маркова и источники с памятью**  
- **Дискретная цепь Маркова**:  
  - Вероятность следующего состояния зависит только от текущего.  
  - Задается матрицей переходных вероятностей \( P_{ij} \).  
- **Энтропия марковского источника**:  
  \[
  H(X) = -\sum_{i,j} \pi_i P_{ij} \log P_{ij},  
  \]  
  где \( \pi_i \) – стационарные вероятности.  

---

## **8. Методы кодирования**  
### **37. Арифметическое кодирование**  
- **Идея**: Кодирование всей последовательности в одно дробное число из интервала \([0, 1)\).  
- **Преимущество**: Эффективнее Хаффмана для малых алфавитов.  

### **38. Кодирование Лемпеля—Зива (LZ-семейство: LZ77, LZ78, LZW)**  
- **Принцип**: Замена повторяющихся подстрок ссылками на предыдущие вхождения.  
- **Применение**: ZIP, GIF, PNG.  

### **39–44. Помехоустойчивые коды**  
#### **39. Коды Хэмминга**  
- **Коррекция одиночных ошибок**.  
- **Пример**: (7,4)-код (4 бита данных + 3 проверочных).  

#### **40. БЧХ-коды (Боуза—Чоудхури—Хоквингема)**  
- **Циклические коды**, исправляющие \( t \) ошибок.  
- **Применение**: QR-коды, спутниковая связь.  

#### **41. Циклические коды**  
- **Особенность**: Любой циклический сдвиг кодового слова – тоже кодовое слово.  
- **Порождающий полином** \( g(x) \).  

#### **42. Свёрточные коды**  
- **Структура**:  
  - Регистр сдвига + сумматоры по модулю 2.  
  - Кодирование зависит от предыдущих битов.  
- **Декодирование**: Алгоритм Витерби.  

#### **43. CRC-коды (Cyclic Redundancy Check)**  
- **Обнаружение ошибок**.  
- **Пример**: Ethernet (CRC-32).  

#### **44. Коды Рида—Соломона**  
- **Особенность**: Работают с блоками символов (не битов).  
- **Исправляют пакеты ошибок**.  
- **Применение**: CD, DVD, DVB.  

---

## **Итоговая структура подготовки**  
1. **Основы**:  
   - Определения информации, энтропии, взаимной информации.  
   - Теоремы Шеннона.  

2. **Кодирование**:  
   - Хаффман, арифметическое, LZ.  
   - Неравенство Крафта.  

3. **Каналы и передача**:  
   - Пропускная способность (дискретные/непрерывные каналы).  
   - Теорема Котельникова.  

4. **Помехоустойчивые коды**:  
   - Хэмминг, БЧХ, Рида—Соломона, свёрточные.  

5. **Дополнительно**:  
   - Цепи Маркова, критерии качества.  

---

### **Пример задачи на код Хэмминга**  
*Закодируйте сообщение 1101 с использованием (7,4)-кода Хэмминга.*  
**Решение**:  
1. Позиции проверочных битов: 1, 2, 4.  
2. Вычисляем:  
   - \( p_1 = d_1 \oplus d_2 \oplus d_4 = 1 \oplus 1 \oplus 1 = 1 \)  
   - \( p_2 = d_1 \oplus d_3 \oplus d_4 = 1 \oplus 0 \oplus 1 = 0 \)  
   - \( p_4 = d_2 \oplus d_3 \oplus d_4 = 1 \oplus 0 \oplus 1 = 0 \)  
3. Кодовое слово: `p1 p2 d1 p4 d2 d3 d4` → **1 0 1 0 1 0 1**.  

---
