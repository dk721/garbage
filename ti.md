## **1. Основные понятия и структурные схемы**  
### **1. Понятие информации. Структурная схема одноканальной системы передачи информации**  
- **Информация** – мера уменьшения неопределенности (по Шеннону).  
- **Структурная схема системы передачи**:  
  ```
  Источник → Кодирующее устройство → Канал связи → Декодирующее устройство → Получатель
  ```  
  (Могут добавляться модуляция, демодуляция, помехи.)  

### **2. Определение дискретных ансамблей и источников сообщений**  
- **Дискретный ансамбль** – множество возможных сообщений с заданными вероятностями.  
- **Источник сообщений** – устройство или процесс, генерирующий сообщения (с памятью/без памяти).  

---

## **2. Меры информации и энтропия**  
### **3. Количество информации в дискретном сообщении**  
- **Формула Шеннона**:  
  ```math
  I(a_i) = -\log_2 P(a_i)
  ```  
  **Обоснование логарифма**: аддитивность для независимых событий.  

### **4. Энтропия ансамбля дискретных сообщений**  
```math
H(X) = -\sum_{i=1}^n P(x_i) \log_2 P(x_i)
```  
**Свойства**:  
- $` H(X) \geq 0 `$  
- Максимальна при равномерном распределении.  

### **5. Условная и совместная энтропия**  
- **Условная**: $` H(X|Y) = -\sum P(x,y) \log P(x|y) `$  
- **Совместная**: $` H(X,Y) = H(X) + H(Y|X) `$  

### **6. Энтропия стационарного источника**  
Для источника с памятью:  
```math
H_\infty = \lim_{n \to \infty} \frac{H(X_1, X_2, ..., X_n)}{n}
```  

### **7. Избыточность источника**  
```math
R = 1 - \frac{H(X)}{H_{max}}
```  
**Безызбыточный источник**: $` H(X) = H_{max} `$ (все символы равновероятны).  

---

## **3. Кодирование информации**  
### **8. Эффективное кодирование**  
Цель: минимизировать среднюю длину кода при условии однозначности декодирования.  

### **9. Равномерное кодирование**  
- Все кодовые слова одинаковой длины (например, ASCII).  

### **10–11. Теорема о высоковероятных множествах. Типичные и нетипичные последовательности**  
- **Типичные последовательности** – их вероятность близка к $` 2^{-nH(X)} `$.  
- **Нетипичные** – все остальные (их вероятность мала).  

### **12. Энтропия как скорость создания информации**  
Для источника без памяти:  
```math
H(X) \text{ [бит/символ]} = \text{скорость генерации информации}
```  

### **13–15. Неравномерные коды**  
- **Код Шеннона-Фано**: разделение вероятностей на две группы.  
- **Код Хаффмана**: оптимальный префиксный код.  
- **Арифметическое кодирование** (Гилберт-Мур): кодирование всей последовательности в одно число.  

---

## **4. Каналы связи и пропускная способность**  
### **16. Классификация каналов**  
- Дискретные/непрерывные, с памятью/без памяти, с помехами/без помех.  

### **17–18. Количество информации и взаимная информация**  
```math
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
```  
**Свойства**:  
- $` I(X;Y) \geq 0 `$  
- Симметричность: $` I(X;Y) = I(Y;X) `$  

### **19. Информационные характеристики системы передачи**  
- **Скорость передачи** $` R `$  
- **Пропускная способность** $` C `$  

### **20. Пропускная способность канала**  
Для дискретного канала без памяти:  
```math
C = \max_{P(x)} I(X;Y)
```  

### **21–22. Симметричные каналы**  
- **Двоичный симметричный канал (ДСК)**:  
  ```math
  C = 1 - H(p), \text{ где } p \text{ – вероятность ошибки}
  ```  
- **ДСК со стиранием**:  
  ```math
  C = 1 - \alpha, \text{ где } \alpha \text{ – вероятность стирания}
  ```  

### **23–25. Теоремы Шеннона о кодировании**  
- **Прямая теорема**: Если $` R < C `$, то существует код с вероятностью ошибки $` \rightarrow 0 `$.  
- **Обратная теорема**: Если $` R > C `$, то надежная передача невозможна.  

### **26. Помехоустойчивое кодирование**  
- **Коды Хэмминга, циклические коды, свёрточные коды**.  

---

## **5. Непрерывные источники и каналы**  
### **27–28. Непрерывные ансамбли**  
- **Дифференциальная энтропия**:  
  ```math
  h(X) = -\int f(x) \log f(x) \, dx
  ```  
- **Взаимная информация для непрерывных сигналов**:  
  ```math
  I(X;Y) = h(X) - h(X|Y)
  ```  

### **29. Пропускная способность непрерывного канала**  
**Формула Шеннона-Хартли**:  
```math
C = B \log_2 \left(1 + \frac{S}{N}\right)
```  
где $` B `$ – полоса пропускания, $` S/N `$ – отношение сигнал/шум.  

---

## **6. Теоремы кодирования и эффективность систем передачи**  
### **30. Теоремы кодирования для непрерывного канала без памяти с дискретным временем**  
- **Прямая теорема**: Если скорость передачи $` R < C `$, то существует код, обеспечивающий сколь угодно малую вероятность ошибки.  
- **Обратная теорема**: Если $` R > C `$, то надёжная передача невозможна.  
- **Аналог теорем Шеннона**, но для непрерывных каналов.  

### **31. Эффективность систем передачи и пределы Шеннона**  
- **Эффективность** – отношение фактической скорости передачи к пропускной способности $` \eta = \frac{R}{C} `$.  
- **Предел Шеннона** – максимальная скорость безошибочной передачи при заданных ограничениях (мощность, полоса).  

### **32. Критерий качества воспроизведения сообщений**  
- **Искажения**: среднеквадратическая ошибка (MSE), вероятность ошибки.  
- **Критерий Найквиста-Котельникова** – точное восстановление при частоте дискретизации $` f_s \geq 2f_{max} `$.  

### **33. Теорема Котельникова (Найквиста-Шеннона)**  
- **Формулировка**:  
  > Аналоговый сигнал с ограниченным спектром $` f_{max} `$ может быть точно восстановлен по дискретным отсчётам, взятым с частотой $` f_s \geq 2f_{max} `$.  

- **Значение**: основа всех цифровых систем связи.  

---

## **7. Коды и неравенства**  
### **34. Неравенство Крафта**  
- **Формулировка**:  
  Для любого префиксного кода с длинами кодовых слов $` l_1, l_2, ..., l_n `$ выполняется:  
  ```math
  \sum_{i=1}^n 2^{-l_i} \leq 1
  ```  
- **Применение**: проверка существования префиксного кода.  

### **35–36. Цепи Маркова и источники с памятью**  
- **Дискретная цепь Маркова**:  
  - Вероятность следующего состояния зависит только от текущего.  
  - Задается матрицей переходных вероятностей $` P_{ij} `$.  
- **Энтропия марковского источника**:  
  ```math
  H(X) = -\sum_{i,j} \pi_i P_{ij} \log P_{ij},  
  ```  
  где $` \pi_i `$ – стационарные вероятности.  

---

## **8. Методы кодирования**  
### **37. Арифметическое кодирование**  
- **Идея**: Кодирование всей последовательности в одно дробное число из интервала $`[0, 1)`$.  
- **Преимущество**: Эффективнее Хаффмана для малых алфавитов.  

### **38. Кодирование Лемпеля—Зива (LZ-семейство: LZ77, LZ78, LZW)**  
- **Принцип**: Замена повторяющихся подстрок ссылками на предыдущие вхождения.  
- **Применение**: ZIP, GIF, PNG.  

### **39–44. Помехоустойчивые коды**  
#### **39. Коды Хэмминга**  
- **Коррекция одиночных ошибок**.  
- **Пример**: (7,4)-код (4 бита данных + 3 проверочных).  

#### **40. БЧХ-коды (Боуза—Чоудхури—Хоквингема)**  
- **Циклические коды**, исправляющие $` t `$ ошибок.  
- **Применение**: QR-коды, спутниковая связь.  

#### **41. Циклические коды**  
- **Особенность**: Любой циклический сдвиг кодового слова – тоже кодовое слово.  
- **Порождающий полином** $` g(x) `$.  

#### **42. Свёрточные коды**  
- **Структура**:  
  - Регистр сдвига + сумматоры по модулю 2.  
  - Кодирование зависит от предыдущих битов.  
- **Декодирование**: Алгоритм Витерби.  

#### **43. CRC-коды (Cyclic Redundancy Check)**  
- **Обнаружение ошибок**.  
- **Пример**: Ethernet (CRC-32).  

#### **44. Коды Рида—Соломона**  
- **Особенность**: Работают с блоками символов (не битов).  
- **Исправляют пакеты ошибок**.  
- **Применение**: CD, DVD, DVB.  

---

## **Итоговая структура подготовки**  
1. **Основы**:  
   - Определения информации, энтропии, взаимной информации.  
   - Теоремы Шеннона.  

2. **Кодирование**:  
   - Хаффман, арифметическое, LZ.  
   - Неравенство Крафта.  

3. **Каналы и передача**:  
   - Пропускная способность (дискретные/непрерывные каналы).  
   - Теорема Котельникова.  

4. **Помехоустойчивые коды**:  
   - Хэмминг, БЧХ, Рида—Соломона, свёрточные.  

5. **Дополнительно**:  
   - Цепи Маркова, критерии качества.  

---

### **Пример задачи на код Хэмминга**  
*Закодируйте сообщение 1101 с использованием (7,4)-кода Хэмминга.*  
**Решение**:  
1. Позиции проверочных битов: 1, 2, 4.  
2. Вычисляем:  
   - $` p_1 = d_1 \oplus d_2 \oplus d_4 = 1 \oplus 1 \oplus 1 = 1 `$  
   - $` p_2 = d_1 \oplus d_3 \oplus d_4 = 1 \oplus 0 \oplus 1 = 0 `$  
   - $` p_4 = d_2 \oplus d_3 \oplus d_4 = 1 \oplus 0 \oplus 1 = 0 `$  
3. Кодовое слово: `p1 p2 d1 p4 d2 d3 d4` → **1 0 1 0 1 0 1**.  

---

## **Шпаргалка по решению задач по теории информации**  

### **1. Количество информации**  
**Формула**:  
```math
I(a_i) = -\log_2 P(a_i) \quad \text{(бит)}
```
**Среднее количество информации (энтропия источника)**:  
```math
H(X) = -\sum_{i=1}^n P(x_i) \log_2 P(x_i)
```

**Пример задачи**:  
*Источник выдаёт символы {A, B, C} с вероятностями P(A) = 0.5, P(B) = 0.3, P(C) = 0.2. Найти энтропию.*  
**Решение**:  
```math
H(X) = - (0.5 \log_2 0.5 + 0.3 \log_2 0.3 + 0.2 \log_2 0.2) \approx 1.485 \text{ бит}
```  

---

### **2. Условная энтропия и взаимная информация**  
**Условная энтропия**:  
```math
H(X|Y) = -\sum_{x,y} P(x,y) \log_2 P(x|y)
```  
**Взаимная информация**:  
```math
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
```  

**Пример задачи**:  
*Даны P(X), P(Y|X). Найти H(X|Y) и I(X;Y).*  
**Алгоритм**:  
1. Найти совместные вероятности $` P(x,y) = P(x) \cdot P(y|x) `$.  
2. Вычислить маргинальные $` P(y) = \sum_x P(x,y) `$.  
3. Найти $` P(x|y) = \frac{P(x,y)}{P(y)} `$.  
4. Подставить в формулы.  

---

### **3. Характеристики каналов связи**  
#### **Дискретный канал без памяти (ДКБП)**  
**Пропускная способность**:  
```math
C = \max_{P(x)} I(X;Y)
```  
**Для двоичного симметричного канала (ДСК)**:  
```math
C = 1 - H(p), \quad \text{где } p \text{ – вероятность ошибки}
```  

#### **Непрерывный канал (Шеннон-Хартли)**  
```math
C = B \log_2 \left(1 + \frac{S}{N}\right)
```  
где $` B `$ – полоса, $` S/N `$ – отношение сигнал/шум.  

**Пример задачи**:  
*Найти пропускную способность ДСК с p = 0.1.*  
**Решение**:  
```math
C = 1 - H(0.1) = 1 - (-0.1 \log_2 0.1 - 0.9 \log_2 0.9) \approx 0.531 \text{ бит/символ}
```  

---

### **4. Эффективное кодирование**  
#### **Код Хаффмана**  
1. Упорядочить символы по убыванию вероятностей.  
2. Объединять два наименее вероятных символа, суммируя их вероятности.  
3. Строить дерево, назначая 0 и 1 на рёбра.  

**Пример**:  
Символы: A(0.5), B(0.3), C(0.2).  
**Код**:  
- A → 0  
- B → 10  
- C → 11  

**Средняя длина**:  
```math
L = 0.5 \cdot 1 + 0.3 \cdot 2 + 0.2 \cdot 2 = 1.5 \text{ бит}
```  

#### **Арифметическое кодирование**  
- Кодирует последовательность в дробь из [0, 1).  

---

### **5. Помехоустойчивое кодирование**  
#### **Код Хэмминга (7,4)**  
1. Позиции проверочных битов: 1, 2, 4.  
2. Данные биты: 3, 5, 6, 7.  
3. Формулы для проверочных битов:  
   - $` p_1 = d_1 \oplus d_2 \oplus d_4 `$  
   - $` p_2 = d_1 \oplus d_3 \oplus d_4 `$  
   - $` p_4 = d_2 \oplus d_3 \oplus d_4 `$  

**Пример**:  
Закодировать 1101.  
**Решение**:  
- $` p_1 = 1 \oplus 1 \oplus 1 = 1 `$  
- $` p_2 = 1 \oplus 0 \oplus 1 = 0 `$  
- $` p_4 = 1 \oplus 0 \oplus 1 = 0 `$  
- Код: **1010101**  

#### **CRC-коды**  
1. Выбрать порождающий полином (например, CRC-8: $` x^8 + x^2 + x + 1 `$).  
2. Дописать к данным $` n `$ нулей (степень полинома).  
3. Разделить (XOR) данные на полином, остаток – CRC.  

---

## **Шпаргалка в таблицах**  

| **Тип задачи**            | **Формула/Алгоритм**                          | **Пример**                          |
|---------------------------|-----------------------------------------------|-------------------------------------|
| **Энтропия**              | $` H(X) = -\sum P(x_i) \log_2 P(x_i) `$      | $` H(0.5, 0.3, 0.2) \approx 1.485 `$ |
| **Код Хаффмана**          | Дерево слияния наименьших вероятностей        | A(0.5)→0, B(0.3)→10, C(0.2)→11      |
| **Пропускная способность**| $` C = \max I(X;Y) `$ (ДКБП)                 | ДСК с $` p=0.1 `$: $` C \approx 0.531 `$ |
| **Код Хэмминга**          | $` p_1 = d_1 \oplus d_2 \oplus d_4 `$        | 1101 → 1010101                      |
| **CRC**                   | Деление с XOR на полином                      | Данные: 110101, полином: 1001 → CRC: 110 |

---

## **Ответы на вопросы для самоконтроля**  

### **1. Что такое информация?**  
**Информация** – мера уменьшения неопределённости (по Шеннону) или данные, несущие смысл.  

### **2. Структурная схема одноканальной системы передачи информации**  
```
Источник → Кодер → Канал связи → Декодер → Получатель  
          (модуляция)       (демодуляция)  
```  
*Дополнительно:* могут быть помехи, шумы, коррекция ошибок.  

### **3. Что такое кодирование сигнала?**  
**Кодирование** – преобразование информации в форму, удобную для передачи/хранения (например, в двоичный код).  

### **4. Какая операция называется декодированием?**  
**Декодирование** – обратное преобразование закодированных данных в исходную форму.  

### **5. Цели кодирования**  
- Сжатие данных (уменьшение избыточности).  
- Защита от ошибок (помехоустойчивое кодирование).  
- Шифрование (конфиденциальность).  

### **6. Определение энтропии**  
**Энтропия $` H(X) `$** – среднее количество информации, приходящееся на одно сообщение:  
```math
H(X) = -\sum_{i=1}^n P(x_i) \log_2 P(x_i)
```  

### **7. Свойства энтропии**  
1. $` H(X) \geq 0 `$ (равно 0, если событие достоверно).  
2. Максимальна при равновероятных исходах.  
3. Аддитивна для независимых источников: $` H(X,Y) = H(X) + H(Y) `$.  

### **8. Единица измерения энтропии**  
- **Бит** (при использовании логарифма по основанию 2).  

### **9. Когда энтропия равна нулю?**  
Если одно из событий имеет вероятность $` P = 1 `$, остальные – $` 0 `$ (нет неопределённости).  

### **10. Когда энтропия максимальна?**  
При равномерном распределении вероятностей:  
```math
H_{max} = \log_2 n, \text{ где } n \text{ – число исходов.}
```  

### **11. Правило сложения энтропий для независимых источников**  
```math
H(X, Y) = H(X) + H(Y)
```  

### **12. Условная энтропия**  
**Условная энтропия $` H(X|Y) `$** – мера неопределённости $` X `$ после наблюдения $` Y `$:  
```math
H(X|Y) = -\sum_{x,y} P(x,y) \log_2 P(x|y)
```  

### **13. Формулы для условной энтропии**  
1. Через совместную энтропию:  
   ```math
   H(X|Y) = H(X,Y) - H(Y)
   ```  
2. Через вероятности:  
   ```math
   H(X|Y) = -\sum_{y} P(y) \sum_{x} P(x|y) \log_2 P(x|y)
   ```  

### **14. Свойства условной энтропии**  
1. $` H(X|Y) \leq H(X) `$ (знание $` Y `$ уменьшает неопределённость).  
2. Если $` X `$ и $` Y `$ независимы: $` H(X|Y) = H(X) `$.  

### **15. Формула Шеннона для количества информации**  
```math
I(a_i) = -\log_2 P(a_i)
```  
*Среднее количество информации* – это энтропия $` H(X) `$.  

### **16. Формула Хартли**  
Для равновероятных событий:  
```math
I = \log_2 N,  
```  
где $` N `$ – число возможных исходов.  

### **17. Коэффициент сжатия**  
```math
K_{сж} = \frac{L_{исх}}{L_{код}},  
```  
где $` L_{исх} `$ – длина исходных данных, $` L_{код} `$ – длина после кодирования.  

### **18. Коэффициент избыточности**  
```math
R = 1 - \frac{H(X)}{H_{max}},  
```  
где $` H_{max} = \log_2 n `$ (максимальная энтропия).  

---

## **Примеры для самопроверки**  
1. **Энтропия**:  
   - Для источника с $` P(A)=0.6, P(B)=0.4 `$:  
     ```math
     H(X) = - (0.6 \log_2 0.6 + 0.4 \log_2 0.4) \approx 0.971 \text{ бит}
     ```  

2. **Условная энтропия**:  
   - Если $` P(X|Y=0) = \{0.7, 0.3\} `$, то:  
     ```math
     H(X|Y=0) = - (0.7 \log_2 0.7 + 0.3 \log_2 0.3) \approx 0.881 \text{ бит}
     ```  

3. **Коэффициент сжатия**:  
   - Исходная строка: 100 байт, после кодирования: 65 байт → $` K_{сж} = \frac{100}{65} \approx 1.54 `$.  

---

### **Условная энтропия и взаимная информация: шпаргалка**  

## **1. Определение условной энтропии**  
**Условная энтропия** $` H(X|Y) `$ – средняя неопределённость случайной величины $` X `$ при известной величине $` Y `$.  
Формула:  
```math
H(X|Y) = -\sum_{x \in X} \sum_{y \in Y} P(x,y) \log_2 P(x|y)
```  
**Смысл**:  
- Сколько "новой" информации несёт $` X `$, если $` Y `$ уже известно.  

---

## **2. Закон аддитивности энтропии (общий случай)**  
Для любых двух случайных величин $` X `$ и $` Y `$:  
```math
H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
```  
**Частные случаи**:  
- Если $` X `$ и $` Y `$ независимы: $` H(X,Y) = H(X) + H(Y) `$.  

---

## **3. Формулы для расчёта условной энтропии**  
1. **Через совместную и маргинальную энтропии**:  
   ```math
   H(X|Y) = H(X,Y) - H(Y)
   ```  
2. **Через условные вероятности**:  
   ```math
   H(X|Y) = -\sum_{y \in Y} P(y) \sum_{x \in X} P(x|y) \log_2 P(x|y)
   ```  

**Пример**:  
Дано:  
- $` P(X,Y) `$:  
  |   | Y=0 | Y=1 |  
  |---|-----|-----|  
  |X=0| 0.4 | 0.1 |  
  |X=1| 0.2 | 0.3 |  
Решение:  
1. $` H(Y) = - (0.6 \log_2 0.6 + 0.4 \log_2 0.4) \approx 0.971 `$ бит.  
2. $` H(X,Y) = - (0.4 \log_2 0.4 + 0.1 \log_2 0.1 + 0.2 \log_2 0.2 + 0.3 \log_2 0.3) \approx 1.846 `$ бит.  
3. $` H(X|Y) = H(X,Y) - H(Y) \approx 1.846 - 0.971 = 0.875 `$ бит.  

---

## **4. Формулы для взаимной информации**  
**Взаимная информация** $` I(X;Y) `$ – мера зависимости $` X `$ и $` Y `$:  
1. **Через энтропии**:  
   ```math
   I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
   ```  
2. **Через совместные и маргинальные вероятности**:  
   ```math
   I(X;Y) = \sum_{x,y} P(x,y) \log_2 \frac{P(x,y)}{P(x)P(y)}
   ```  

**Свойства**:  
- $` I(X;Y) \geq 0 `$ (равно 0, если $` X `$ и $` Y `$ независимы).  
- Симметричность: $` I(X;Y) = I(Y;X) `$.  

**Пример**:  
Для предыдущих данных:  
```math
I(X;Y) = H(X) - H(X|Y) \approx 1.0 - 0.875 = 0.125 \text{ бит}.
```  

---

## **5. Полная средняя взаимная информация**  
Для ансамбля из $` N `$ сообщений:  
```math
I_{\text{полн}}(X;Y) = N \cdot I(X;Y)
```  
**Применение**:  
- Характеризует общее количество информации, переданное за $` N `$ шагов.  

---

## **6. Дискретные системы передачи информации**  
**Определение**:  
- Системы, где сообщения и сигналы принимают дискретные значения (например, двоичные коды).  
**Примеры**:  
- Цифровая связь (Wi-Fi, мобильные сети).  
- Дискретные каналы (Двоичный симметричный канал).  

---

## **7. Непрерывные системы передачи информации**  
**Определение**:  
- Системы, где сообщения и сигналы описываются непрерывными функциями (аналоговые сигналы).  
**Примеры**:  
- Аналоговое радио, телефония.  
- Гауссовы каналы.  

---

## **8. Условная энтропия в непрерывных системах**  
Для непрерывных случайных величин $` X `$ и $` Y `$:  
```math
h(X|Y) = -\iint f(x,y) \log_2 f(x|y) \, dx dy
```  
где:  
- $` f(x,y) `$ – совместная плотность вероятности,  
- $` f(x|y) `$ – условная плотность.  

**Связь с дифференциальной энтропией**:  
```math
h(X|Y) = h(X,Y) - h(Y)
```  

**Важно**:  
- В непрерывном случае энтропия может быть отрицательной!  

---

## **Итоговая таблица**  

| **Понятие**               | **Формула**                                  | **Пример применения**              |
|---------------------------|---------------------------------------------|------------------------------------|
| Условная энтропия         | $` H(X|Y) = H(X,Y) - H(Y) `$               | Оценка информации после приёма $` Y `$ |
| Взаимная информация       | $` I(X;Y) = H(X) - H(X|Y) `$              | Измерение зависимости $` X `$ и $` Y `$ |
| Полная взаимная информация| $` I_{\text{полн}} = N \cdot I(X;Y) `$     | Расчёт для последовательности сообщений |
| Непрерывная условная энтропия | $` h(X|Y) = h(X,Y) - h(Y) `$          | Аналоговые системы связи           |

---

**Как применять**:  
1. Для дискретных задач – используйте суммы и вероятности.  
2. Для непрерывных – интегралы и плотности распределения.  
3. Взаимная информация полезна для анализа каналов связи.  

**Пример для непрерывного случая**:  
Если $` X `$ и $` Y `$ – гауссовы величины с корреляцией $` \rho `$:  
```math
h(X|Y) = \frac{1}{2} \log_2 (2 \pi e \sigma_X^2 (1 - \rho^2))
```  

---

### **Информационная мера Шеннона. Количество информации и избыточность**  

## **1. Определение энтропии**  
**Энтропия (по Шеннону)** – средняя мера неопределённости или информационной содержательности источника сообщений.  
**Формула для дискретного источника**:  
```math
H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
```  
где:  
- $` P(x_i) `$ – вероятность $` i `$-го сообщения,  
- $` n `$ – количество возможных сообщений.  

**Смысл**:  
- Чем выше энтропия, тем больше неопределённость или информативность источника.  

---

## **2. Формула Шеннона для количества информации**  
**Количество информации** в отдельном сообщении $` x_i `$:  
```math
I(x_i) = -\log_2 P(x_i)
```  
**Среднее количество информации (энтропия)**:  
```math
H(X) = \sum_{i=1}^{n} P(x_i) I(x_i) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
```  

**Пример**:  
Для источника с $` P(A) = 0.5 `$, $` P(B) = 0.3 `$, $` P(C) = 0.2 `$:  
```math
H(X) = - (0.5 \log_2 0.5 + 0.3 \log_2 0.3 + 0.2 \log_2 0.2) \approx 1.485 \text{ бит}
```  

---

## **3. Формула Хартли**  
Используется для **равновероятных событий**:  
```math
I = \log_2 N
```  
где $` N `$ – число возможных равновероятных исходов.  

**Пример**:  
Для 8 равновероятных сообщений:  
```math
I = \log_2 8 = 3 \text{ бита}
```  

---

## **4. Основные свойства энтропии**  
1. **Неотрицательность**: $` H(X) \geq 0 `$.  
2. **Максимальность**:  
   - Энтропия максимальна при равномерном распределении: $` H_{\text{max}} = \log_2 n `$.  
3. **Аддитивность**:  
   - Для независимых источников: $` H(X,Y) = H(X) + H(Y) `$.  
4. **Выпуклость**: $` H(X) `$ максимальна при равных вероятностях.  

---

## **5. Единица измерения энтропии**  
- **Бит** (при логарифме по основанию 2),  
- **Нат** (при натуральном логарифме),  
- **Хартли** (при логарифме по основанию 10).  

---

## **6. Когда энтропия равна нулю?**  
Если одно из событий имеет вероятность $` P = 1 `$, а остальные – $` 0 `$.  
**Пример**:  
Источник всегда выдаёт символ $` A `$ ($` P(A) = 1 `$):  
```math
H(X) = -1 \log_2 1 = 0 \text{ бит}
```  

---

## **7. Условия максимальной энтропии**  
Энтропия максимальна, когда все события **равновероятны**:  
```math
H_{\text{max}} = \log_2 n
```  
**Пример**:  
Для источника с 4 символами:  
```math
H_{\text{max}} = \log_2 4 = 2 \text{ бита}
```  

---

## **8. Правило сложения энтропий для независимых источников**  
Если $` X `$ и $` Y `$ независимы:  
```math
H(X,Y) = H(X) + H(Y)
```  
**Пример**:  
- $` H(X) = 1 `$ бит (для одного независимого бита),  
- $` H(Y) = 1 `$ бит,  
- Тогда $` H(X,Y) = 1 + 1 = 2 `$ бита.  

---

## **9. Количество информации для непрерывных сообщений**  
Для непрерывной случайной величины $` X `$ с плотностью распределения $` f(x) `$:  
- **Дифференциальная энтропия**:  
  ```math
  h(X) = -\int_{-\infty}^{\infty} f(x) \log_2 f(x) \, dx
  ```  
- **Взаимная информация** (между $` X `$ и $` Y `$):  
  ```math
  I(X;Y) = h(X) - h(X|Y)
  ```  

**Важно**:  
- Дифференциальная энтропия может быть отрицательной!  

---

## **10. Формула избыточности**  
**Избыточность** $` R `$ – мера неоптимальности кодирования:  
```math
R = 1 - \frac{H(X)}{H_{\text{max}}}
```  
где:  
- $` H(X) `$ – текущая энтропия источника,  
- $` H_{\text{max}} = \log_2 n `$ – максимальная энтропия (при равномерном распределении).  

**Пример**:  
Для источника с $` H(X) = 1.5 `$ бита и $` H_{\text{max}} = 2 `$ бита:  
```math
R = 1 - \frac{1.5}{2} = 0.25 \quad \text{(25%)}
```  

---

## **Итоговая таблица**  

| **Понятие**               | **Формула**                                  | **Пример**                          |
|---------------------------|---------------------------------------------|-------------------------------------|
| Энтропия (Шеннон)         | $` H(X) = -\sum P(x_i) \log_2 P(x_i) `$    | $` H(0.5, 0.3, 0.2) \approx 1.485 `$ бит |
| Формула Хартли            | $` I = \log_2 N `$                          | Для 8 исходов: $` \log_2 8 = 3 `$ бита |
| Максимальная энтропия     | $` H_{\text{max}} = \log_2 n `$             | Для 4 символов: 2 бита              |
| Избыточность              | $` R = 1 - \frac{H(X)}{H_{\text{max}}} `$   | При $` H=1.5 `$, $` H_{\text{max}}=2 `$: $` R=0.25 `$ |

---

**Ключевые выводы**:  
- Энтропия измеряет **неопределённость** или **информационную ёмкость**.  
- Избыточность показывает, насколько источник далёк от оптимального кодирования.  
- Для непрерывных сигналов используют **дифференциальную энтропию**.  

---

### **Передача информации по каналу связи. Пропускная способность канала**  

## **1. Техническая скорость (символьная скорость)**  
**Определение**:  
- Количество символов (элементарных сигналов), передаваемых в единицу времени.  
**Формула**:  
```math
V_{\text{техн}} = \frac{N_{\text{симв}}}{T} \quad \text{[симв/сек, Бод]}
```  
где:  
- $` N_{\text{симв}} `$ – число переданных символов,  
- $` T `$ – время передачи.  

**Пример**:  
Если за 1 секунду передаётся 1000 импульсов, то $` V_{\text{техн}} = 1000 `$ Бод.  

---

## **2. Информационная скорость (скорость передачи информации)**  
**Определение**:  
- Количество информации (в битах), передаваемое в единицу времени.  
**Формула**:  
```math
R = \frac{I}{T} \quad \text{[бит/сек]}
```  
где $` I `$ – количество информации в сообщении.  

**Для дискретного канала**:  
```math
R = V_{\text{техн}} \cdot H(X) \quad \text{(если символы зависимы)}
```  
или  
```math
R = V_{\text{техн}} \cdot \log_2 m \quad \text{(для равновероятных независимых символов)}
```  
где $` m `$ – число различных символов.  

**Пример**:  
- Техническая скорость: 1000 Бод,  
- Энтропия на символ: $` H(X) = 1.5 `$ бит,  
- Тогда $` R = 1000 \times 1.5 = 1500 `$ бит/сек.  

---

## **3. Информационная скорость для равновероятных сообщений**  
Если все $` m `$ символов равновероятны и независимы:  
```math
R = V_{\text{техн}} \cdot \log_2 m
```  
**Пример**:  
- 8-уровневая модуляция ($` m = 8 `$),  
- $` V_{\text{техн}} = 1000 `$ Бод,  
- Тогда $` R = 1000 \times \log_2 8 = 3000 `$ бит/сек.  

---

## **4. Пропускная способность канала без помех**  
**Определение**:  
- Максимальная скорость передачи информации при отсутствии ошибок.  
**Формула**:  
```math
C = V_{\text{техн}} \cdot \log_2 m \quad \text{[бит/сек]}
```  
**Пример**:  
Для двоичного канала ($` m = 2 `$) с $` V_{\text{техн}} = 2000 `$ Бод:  
```math
C = 2000 \times \log_2 2 = 2000 \text{ бит/сек}
```  

---

## **5. Пропускная способность канала с помехами**  
**Формула Шеннона для дискретного канала**:  
```math
C = \max_{P(x)} I(X;Y) \quad \text{[бит/символ]}
```  
где $` I(X;Y) `$ – взаимная информация.  

**Для двоичного симметричного канала (ДСК)**:  
```math
C = 1 - H(p) \quad \text{[бит/символ]}
```  
где $` p `$ – вероятность ошибки.  

**Для непрерывного канала с шумом (теорема Шеннона-Хартли)**:  
```math
C = B \cdot \log_2 \left(1 + \frac{S}{N}\right) \quad \text{[бит/сек]}
```  
где:  
- $` B `$ – полоса пропускания [Гц],  
- $` S/N `$ – отношение сигнал/шум (по мощности).  

**Пример**:  
- Полоса $` B = 1 `$ МГц,  
- $` S/N = 100 `$ (20 дБ),  
- Тогда $` C = 10^6 \times \log_2 (1 + 100) \approx 6.66 `$ Мбит/сек.  

---

## **6. Первая теорема Шеннона (о кодировании без шума)**  
**Формулировка**:  
> Для источника с энтропией $` H(X) `$ и канала с пропускной способностью $` C `$ существует способ кодирования, позволяющий передавать информацию со скоростью $` R `$, сколь угодно близкой к $` C `$, если $` H(X) \leq C `$.  

**Следствие**:  
- Если $` H(X) > C `$, безошибочная передача невозможна.  

---

## **7. Вторая теорема Шеннона (о кодировании при наличии шума)**  
**Формулировка**:  
> Для канала с пропускной способностью $` C `$ и скоростью передачи $` R < C `$ существует код, обеспечивающий сколь угодно малую вероятность ошибки.  

**Обратное утверждение**:  
> Если $` R > C `$, то никакой код не сможет обеспечить надёжную передачу.  

**Пример**:  
- Для ДСК с $` C = 0.5 `$ бит/символ можно найти код, позволяющий передавать данные со скоростью $` R = 0.4 `$ бит/символ и вероятностью ошибки $` P_{\text{ош}} \to 0 `$.  

---

## **Итоговая таблица**  

| **Понятие**                     | **Формула**                                  | **Пример**                          |
|---------------------------------|---------------------------------------------|-------------------------------------|
| Техническая скорость            | $` V_{\text{техн}} = \frac{N_{\text{симв}}}{T} `$ | 1000 Бод                           |
| Информационная скорость         | $` R = V_{\text{техн}} \cdot H(X) `$       | 1500 бит/сек ($` H=1.5 `$, $` V=1000 `$) |
| Пропускная способность (без шума) | $` C = V_{\text{техн}} \cdot \log_2 m `$ | 2000 бит/сек ($` m=2 `$, $` V=2000 `$) |
| Пропускная способность (с шумом) | $` C = B \log_2 (1 + S/N) `$              | 6.66 Мбит/сек ($` B=1 `$ МГц, $` S/N=100 `$) |
| 1-я теорема Шеннона             | $` H(X) \leq C `$ → безошибочная передача | Источник с $` H=1 `$ бит/символ, $` C=1.2 `$ → возможно |
| 2-я теорема Шеннона             | $` R < C `$ → $` P_{\text{ош}} \to 0 `$   | $` R=0.4 `$, $` C=0.5 `$ → возможно |

---

**Ключевые выводы**:  
1. **Техническая скорость** измеряется в Бодах, **информационная** – в битах в секунду.  
2. **Пропускная способность** зависит от:  
   - Для дискретных каналов: числа символов и вероятности ошибки.  
   - Для непрерывных: полосы частот и отношения сигнал/шум.  
3. **Теоремы Шеннона** задают фундаментальные пределы передачи информации.  

---

### **Эффективное кодирование: шпаргалка**  

## **1. Оптимальное (эффективное) кодирование**  
**Определение**:  
Кодирование, при котором **средняя длина кода** минимальна для данного распределения вероятностей.  
**Критерий**:  
```math
L_{\text{ср}} = \sum P(x_i) l_i \to \min
```  
где $` l_i `$ – длина кодового слова для символа $` x_i `$.  

---

## **2. Префиксный код**  
**Определение**:  
Код, в котором **никакое кодовое слово не является префиксом другого**.  
**Свойства**:  
- Позволяет однозначно декодировать сообщение без разделителей.  
- Пример: коды Хаффмана, Шеннона-Фано.  

**Пример**:  
- Допустимо: `{0, 10, 11}` (непересекающиеся префиксы).  
- Недопустимо: `{0, 01, 11}` ("0" – префикс "01").  

---

## **3. Вектор Крафта и неравенство Крафта**  
- **Вектор Крафта**: Набор длин кодовых слов $` (l_1, l_2, ..., l_n) `$.  
- **Неравенство Крафта**:  
  ```math
  \sum_{i=1}^n 2^{-l_i} \leq 1
  ```  
**Условие**:  
Неравенство выполняется **тогда и только тогда**, когда существует префиксный код с заданными длинами.  

**Пример**:  
Для длин $` \{1, 2, 3\} `$:  
```math
2^{-1} + 2^{-2} + 2^{-3} = 0.875 \leq 1 \quad \text{→ код существует.}
```  

---

## **4. Методика построения кода Шеннона**  
**Шаги**:  
1. Упорядочить символы по убыванию вероятностей.  
2. Найти кумулятивные вероятности $` Q_i = \sum_{k=1}^{i-1} P(x_k) `$.  
3. Длина кода: $` l_i = \lceil -\log_2 P(x_i) \rceil `$.  
4. Перевести $` Q_i `$ в двоичную дробь и взять первые $` l_i `$ бит.  

**Пример**:  
Для $` P(A)=0.5 `$, $` P(B)=0.3 `$, $` P(C)=0.2 `$:  
- $` Q_A = 0 `$, $` l_A = 1 `$ → код: `0`.  
- $` Q_B = 0.5 `$, $` l_B = 2 `$ → $` 0.5_{10} = 0.1_2 `$ → код: `10`.  
- $` Q_C = 0.8 `$, $` l_C = 3 `$ → $` 0.8_{10} \approx 0.110_2 `$ → код: `110`.  

---

## **5. Методика построения кода Шеннона-Фано**  
**Шаги**:  
1. Упорядочить символы по убыванию вероятностей.  
2. Разделить набор на две группы с примерно равными суммарными вероятностями.  
3. Первой группе присвоить `0`, второй – `1`.  
4. Рекурсивно повторить для каждой подгруппы.  

**Пример**:  
Для $` P(A)=0.4 `$, $` P(B)=0.3 `$, $` P(C)=0.2 `$, $` P(D)=0.1 `$:  
1. Группы: `{A}, {B,C,D}` → коды: `A=0`, остальные `1...`.  
2. Делим `{B,C,D}` на `{B}, {C,D}` → `B=10`, `C=110`, `D=111`.  

---

## **6. Методика построения кода Хаффмана**  
**Шаги**:  
1. Упорядочить символы по возрастанию вероятностей.  
2. Объединить два символа с наименьшими вероятностями в один узел (сумма вероятностей).  
3. Повторять, пока не останется один корневой узел.  
4. Назначить `0` и `1` на рёбра дерева.  

**Пример**:  
Для $` P(A)=0.5 `$, $` P(B)=0.3 `$, $` P(C)=0.2 `$:  
- Объединяем $` B `$ и $` C `$ (сумма 0.5).  
- Коды: `A=0`, `B=10`, `C=11`.  

---

## **7. Методика построения кода Гилберта-Мура (арифметическое кодирование)**  
**Идея**:  
- Кодирует **всю последовательность** в одно дробное число из интервала $`[0, 1)`$.  
**Шаги**:  
1. Разбить интервал $`[0, 1)`$ на подынтервалы по вероятностям символов.  
2. Для каждого символа последовательности сужать текущий интервал.  
3. Выбрать число из финального интервала (обычно его двоичную запись).  

**Пример**:  
Для $` P(A)=0.6 `$, $` P(B)=0.4 `$ и последовательности `AAB`:  
1. Исходный интервал: $`[0, 1)`$.  
2. После `A`: $`[0, 0.6)`$.  
3. После `AA`: $`[0, 0.36)`$.  
4. После `AAB`: $`[0.216, 0.36)`$.  
5. Код: число $` 0.28 `$ (например, `0.010001...` в двоичной форме).  

---

## **8. Теорема Котельникова (Найквиста-Шеннона)**  
**Формулировка**:  
> Аналоговый сигнал с ограниченной полосой частот $` f_{\text{max}} `$ может быть **точно восстановлен** по дискретным отсчётам, взятым с частотой:  
```math
f_s \geq 2 f_{\text{max}}
```  

**Пример**:  
Для звука с $` f_{\text{max}} = 20 `$ кГц частота дискретизации должна быть $` \geq 40 `$ кГц.  

---

## **9. Оптимальный код**  
**Определение**:  
Код, имеющий **минимальную среднюю длину** для заданного распределения вероятностей.  
**Критерии**:  
1. Выполнено неравенство Крафта.  
2. Длины кодовых слов обратно пропорциональны вероятностям: $` l_i \approx -\log_2 P(x_i) `$.  

**Примеры оптимальных кодов**:  
- Код Хаффмана,  
- Арифметическое кодирование (Гилберта-Мура).  

---

## **Итоговая таблица методов кодирования**  

| **Метод**           | **Тип кода**       | **Оптимальность** | **Пример**              |
|---------------------|--------------------|-------------------|-------------------------|
| Шеннона             | Префиксный         | Нет               | `0`, `10`, `110`        |
| Шеннона-Фано        | Префиксный         | Частично          | `0`, `10`, `110`, `111` |
| Хаффмана            | Префиксный         | Да                | `0`, `10`, `11`         |
| Гилберта-Мура       | Непрерывный        | Да (асимптотически)| Дробное число в $`[0,1)`$ |

---

**Ключевые выводы**:  
1. **Оптимальный код** минимизирует $` L_{\text{ср}} `$ и удовлетворяет неравенству Крафта.  
2. **Префиксные коды** позволяют однозначное декодирование.  
3. **Код Хаффмана** – оптимальный префиксный код для дискретных источников.  
4. **Арифметическое кодирование** эффективнее для малых алфавитов и зависимых символов.  

---

### **Помехоустойчивое кодирование: шпаргалка**  

## **1. Помехоустойчивые коды**  
**Определение**:  
Коды, которые позволяют **обнаруживать и исправлять ошибки** при передаче данных за счёт **избыточности**.  

**Типы**:  
- **Обнаруживающие ошибки** (CRC, контрольные суммы).  
- **Исправляющие ошибки** (коды Хэмминга, БЧХ, Рида-Соломона).  

---

## **2. Избыточность**  
**Определение**:  
Добавление **дополнительных битов** к исходным данным для контроля и коррекции ошибок.  

**Формула**:  
```math
R = \frac{n - k}{n},  
```  
где:  
- $` n `$ – длина кодового слова,  
- $` k `$ – число информационных битов.  

**Пример**:  
В коде Хэмминга (7,4):  
```math
R = \frac{7 - 4}{7} \approx 0.43 \quad (43\%).  
```  

---

## **3. Корректирующие коды**  
**Принцип**:  
- К исходным $` k `$ битам добавляют $` r `$ проверочных битов ($` n = k + r `$).  
- Проверочные биты вычисляются по определённым правилам (например, чётность).  

**Пример**:  
Код Хэмминга, БЧХ, свёрточные коды.  

---

## **4. Построение кода Хэмминга**  
**Шаги**:  
1. Определить число проверочных битов $` r `$:  
   ```math
   2^r \geq k + r + 1.  
   ```  
2. Разместить проверочные биты на позициях $` 2^0, 2^1, 2^2, ... `$ (1, 2, 4, 8, ...).  
3. Вычислить каждый проверочный бит как **XOR** определённых информационных битов.  

**Пример для кода (7,4)**:  
- Информационные биты: $` d_3, d_5, d_6, d_7 `$.  
- Проверочные биты:  
  ```math
  \begin{cases}
  p_1 = d_3 \oplus d_5 \oplus d_7, \\
  p_2 = d_3 \oplus d_6 \oplus d_7, \\
  p_4 = d_5 \oplus d_6 \oplus d_7.
  \end{cases}
  ```  

---

## **5. Основные параметры кода Хэмминга**  
- $` n `$ – общая длина кода (например, 7).  
- $` k `$ – число информационных битов (например, 4).  
- $` r `$ – число проверочных битов ($` r = n - k `$, например, 3).  
- **Минимальное расстояние** $` d_{\text{min}} = 3 `$ (обнаруживает 2 ошибки, исправляет 1).  

---

## **6. Общее число элементов кодовой комбинации**  
Для кода Хэмминга:  
```math
n = 2^r - 1,  
```  
где $` r `$ – число проверочных битов.  

**Пример**:  
Если $` r = 3 `$, то $` n = 7 `$.  

---

## **7. Число проверочных и информационных битов**  
- **Проверочные биты**: $` r `$, где $` 2^r \geq n + 1 `$.  
- **Информационные биты**: $` k = n - r `$.  

**Пример**:  
Для $` n = 7 `$:  
```math
2^r \geq 8 \Rightarrow r = 3, \quad k = 4.
```  

---

## **8. Номера проверочных позиций**  
Проверочные биты размещаются на позициях **степеней двойки**:  
```math
1, 2, 4, 8, 16, ...
```  

**Пример для (7,4)**:  
Позиции: 1 ($` p_1 `$), 2 ($` p_2 `$), 4 ($` p_4 `$).  

---

## **9. Закон расчёта контрольных символов**  
Каждый проверочный бит контролирует **определённые информационные биты**:  
- $` p_1 `$: биты, где в двоичном представлении **установлен 1-й разряд** (1, 3, 5, 7).  
- $` p_2 `$: биты с **установленным 2-м разрядом** (2, 3, 6, 7).  
- $` p_4 `$: биты с **установленным 3-м разрядом** (4, 5, 6, 7).  

---

## **10. Правило чётности**  
- Проверочный бит выбирается так, чтобы сумма контролируемых битов (включая проверочный) была **чётной**.  
- Вычисляется операцией **XOR**.  

**Пример**:  
Для $` p_1 = d_3 \oplus d_5 \oplus d_7 `$, если сумма нечётная, $` p_1 = 1 `$.  

---

## **11. Переход из двоичной в десятичную систему**  
**Метод**:  
```math
\text{Десятичное число} = \sum_{i=0}^{n} b_i \cdot 2^i,  
```  
где $` b_i `$ – биты двоичного числа (справа налево).  

**Пример**:  
`1011` → $` 1 \cdot 2^0 + 1 \cdot 2^1 + 0 \cdot 2^2 + 1 \cdot 2^3 = 11 `$.  

---

## **12. Особенности кода Хэмминга**  
- Исправляет **одиночные ошибки** и обнаруживает **двойные**.  
- Минимальное расстояние $` d_{\text{min}} = 3 `$.  
- Проверочные биты на степенях двойки.  

---

## **13. Синдром**  
**Определение**:  
Вектор, показывающий **наличие и местоположение ошибки**. Вычисляется как XOR принятых проверочных битов и пересчитанных.  

**Формула**:  
```math
S = (s_1, s_2, s_4), \quad \text{где } s_i = p_i \oplus p_i'.  
```  
- Если $` S = 0 `$ – ошибок нет.  
- Если $` S \neq 0 `$ – номер ошибочного бита: $` s_1 + s_2 \cdot 2 + s_4 \cdot 4 `$.  

**Пример**:  
Если $` S = (1, 1, 0) `$, ошибка в бите $` 1 + 2 = 3 `$.  

---

## **14. Процедура исправления ошибок**  
1. Вычислить синдром $` S `$.  
2. Если $` S \neq 0 `$, найти позицию ошибки: $` \text{позиция} = \sum s_i \cdot 2^{i-1} `$.  
3. Инвертировать бит в найденной позиции.  

**Пример**:  
Принято: `1011001` (ошибка в 3-м бите → `1001001`).  
- Синдром: $` S = (1, 1, 0) `$ → ошибка в бите 3.  
- Исправление: инвертируем бит 3 → `1011001`.  

---

## **Итоговая таблица**  

| **Параметр**               | **Формула/Правило**                          | **Пример**                    |
|----------------------------|---------------------------------------------|-------------------------------|
| Число проверочных битов    | $` 2^r \geq k + r + 1 `$                    | Для $` k=4 `$: $` r=3 `$      |
| Позиции проверочных битов  | Степени двойки: 1, 2, 4, 8, ...             | В (7,4)-коде: 1, 2, 4         |
| Синдром                    | $` S = (p_1 \oplus p_1', p_2 \oplus p_2', ...) `$ | $` S=(1,1,0) `$ → бит 3      |
| Избыточность               | $` R = \frac{r}{n} `$                        | Для (7,4): $` R \approx 43\% `$ |

---

**Ключевые выводы**:  
1. Код Хэмминга – **простой и эффективный** для исправления одиночных ошибок.  
2. **Синдром** позволяет локализовать ошибку.  
3. **Избыточность** – плата за помехоустойчивость.  

---
